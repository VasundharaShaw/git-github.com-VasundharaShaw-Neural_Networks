{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Pytorch MNIST digits example from https://machinelearningmastery.com/handwritten-digit-recognition-with-lenet5-model-in-pytorch/\n",
    "# # THIS IS ME PLAYING WITH a relatively large MLP for fun\n",
    "# # SEEMS TO WORK WELL > 98% ACCURACY!!!\n",
    "\n",
    "\n",
    "# # Imports\n",
    "import torch # Main PyTorch module\n",
    "import torch.nn as nn # Neural Network module\n",
    "import torch.optim as optim  # Optimization module\n",
    "import torchvision # Computer Vision module\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.youtube.com/watch?v=lGLto9Xd7bU&list=PLQVvvaa0QuDcjD5BAw2DxE6OF2tius3V3&index=2 - Reference "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Convert the above to object oriented code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 1 output = \n",
      " [[-0.25019707  0.29621499 -0.44316805  0.26722439 -0.07514709]\n",
      " [-0.0591695   0.61932802 -0.37870028  1.13393833 -0.65706718]\n",
      " [-0.19564971 -0.13415211 -0.14491714 -0.33268666 -0.06286878]]\n",
      "Layer 2 output = \n",
      " [[ 0.01619024  0.04631842]\n",
      " [-0.17965935  0.02002777]\n",
      " [ 0.04101197  0.05067731]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "X = [[1,2,3,2.5],[2.0,5.0,-1.0,2.0],[ -1.5,2.7,3.3,-0.8]] ## 3 sets of inputs data \n",
    "\n",
    "## Create Layer class \n",
    "class Layer_Dense: \n",
    "    def __init__(self, n_inputs, n_neurons): ## n_inputs = number of inputs to the layer, n_neurons = number of neurons in the layer\n",
    "        ## Initialize weights and biases\n",
    "        self.weights = 0.10 * np.random.randn(n_inputs, n_neurons) ## Random weights with small values. This is useful for forward pass and we do not need to transpose weights here.\n",
    "        self.biases = np.zeros((1, n_neurons)) ## Biases should be initialized to zero values\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        ## Forward pass\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "\n",
    "\n",
    "\n",
    "## Create first layer\n",
    "layer1 = Layer_Dense(n_inputs=4, n_neurons=5) ## First layer with 4 inputs and 5 neurons\n",
    "layer2 = Layer_Dense(n_inputs=5, n_neurons=2) ## Second layer with 5 inputs (from previous layer) and 2 neurons\n",
    "## Forward pass through the layers\n",
    "layer1.forward(X) ## Forward pass through first layer. We will now have layer1.output value\n",
    "print(\"Layer 1 output = \\n\", layer1.output) ## Print output of first layer\n",
    "layer2.forward(layer1.output) ## Forward pass through second layer. We will now have layer2.output value which is calculated from output of first layer\n",
    "print(\"Layer 2 output = \\n\", layer2.output) ## Print output of second layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation function --> https://www.youtube.com/watch?v=gmjzbpSVY1A&list=PLQVvvaa0QuDcjD5BAw2DxE6OF2tius3V3&index=5 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Why do we use activation functions?\n",
    "\n",
    "# Ans. Activation functions introduce non-linearity into the neural network, allowing it to learn and model complex patterns in the data. \n",
    "# Without activation functions, a neural network would simply be a linear combination of its inputs, limiting its ability to capture intricate relationships. \n",
    "# Non-linear activation functions enable the network to approximate any continuous function, making it more powerful and capable of solving a wide range of tasks, such as image recognition, natural language processing, and more.\n",
    "# Some common activation functions include ReLU (Rectified Linear Unit), Sigmoid, and Tanh.\n",
    "# So with linear activation function you can only fit linear data but with non linear activation function you can fit complex non linear data also."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Step function or Heaviside step function as an activation function. \n",
    "# Activation function is applied to the output of the layer to introduce non-linearity i.e. it comes into play after the forward pass of the layer so when we have done inputs * weights + biases.\n",
    "# Step function outputs 1 if input is greater than 0 else it outputs 0.\n",
    "# This is a simple activation function and is not commonly used in practice but is useful for understanding the concept of activation functions.\n",
    "# The output of the activation function is then passed to the next layer as input. \n",
    "# We need activation functions to introduce non-linearity in the model so that it can learn complex patterns in the data.\n",
    "\n",
    "\n",
    "# Another activation function is the sigmoid function which outputs values between 0 and 1.\n",
    "# It is defined as 1 / (1 + exp(-x)) where x is the input to the function.\n",
    "# Sigmoid function is better than step function as it is differentiable and smooth and calculates loss better.\n",
    "# Sigmoid has the problem of vanishing gradient for very high or very low values of input where the gradient becomes very small and the model stops learning.\n",
    "\n",
    "# Another activation function is the Relu function.\n",
    "# The rectified linear unit (ReLU) is another popular activation function. \n",
    "# It outputs the input directly if it is positive else it outputs 0. The formula is max(0, x). \n",
    "# It is computationally efficient and helps in reducing the likelihood of vanishing gradient problem because it does not saturate for positive values.\n",
    "# ReLU is widely used in deep learning models and has been shown to work well in practice.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlproj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
