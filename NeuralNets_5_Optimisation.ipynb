{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Pytorch MNIST digits example from https://machinelearningmastery.com/handwritten-digit-recognition-with-lenet5-model-in-pytorch/\n",
    "# # THIS IS ME PLAYING WITH a relatively large MLP for fun\n",
    "# # SEEMS TO WORK WELL > 98% ACCURACY!!!\n",
    "\n",
    "\n",
    "# # Imports\n",
    "import torch # Main PyTorch module\n",
    "import torch.nn as nn # Neural Network module\n",
    "import torch.optim as optim  # Optimization module\n",
    "import torchvision # Computer Vision module\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "import numpy as np\n",
    "import time\n",
    "import math\n",
    "import nnfs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Calculating loss function\n",
    "\n",
    "## Categorical cross-entropy loss function L = - log(p) where p is the predicted probability of the true class (natural logarithm to base e)\n",
    "# Loss for one hot vector: L = - sum(y_true * log(y_pred)) where y_true is the one hot vector and y_pred is the predicted probability vector\n",
    "\n",
    "# What is one hot vector?\n",
    "# A one hot vector is a vector where the index of the true class is 1 and all other indices are 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  0.35667494393873245\n",
      "Loss simplified:  0.35667494393873245\n",
      "Loss over simplified calculation matches full calculation:  0.35667494393873245\n"
     ]
    }
   ],
   "source": [
    "softmax_outputs = np.array([0.7, 0.1, 0.2])\n",
    "\n",
    "target_outputs = np.array([1, 0, 0] ) # One hot vector for class 0\n",
    "\n",
    "loss = -(math.log(softmax_outputs[0]) * target_outputs[0] + #\n",
    "            math.log(softmax_outputs[1]) * target_outputs[1] +\n",
    "            math.log(softmax_outputs[2]) * target_outputs[2])\n",
    "\n",
    "print('Loss: ', loss)  # Should print 0.35667494393873245\n",
    "\n",
    "\n",
    "loss = -np.log(softmax_outputs[0])  # Since only the first term is non-zero\n",
    "\n",
    "print('Loss simplified: ', loss)  # Should print 0.35667494393873245\n",
    "\n",
    "print('Loss over simplified calculation matches full calculation: ',-math.log(0.7) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First layer output after ReLU activation = \n",
      " [[0.         0.         0.        ]\n",
      " [0.         0.00113954 0.        ]\n",
      " [0.         0.00317292 0.        ]\n",
      " [0.         0.00526663 0.        ]\n",
      " [0.         0.00714014 0.        ]]\n",
      "\n",
      "\n",
      "Second layer output after Softmax activation = \n",
      " [[0.33333334 0.33333334 0.33333334]\n",
      " [0.33334687 0.33334196 0.3333112 ]\n",
      " [0.3333612  0.3333536  0.33328524]\n",
      " [0.33336097 0.33335987 0.33327916]\n",
      " [0.33337367 0.3333704  0.33325592]]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from nnfs.datasets import spiral_data\n",
    "np.random.seed(0) ## Set random seed for reproducibility\n",
    "\n",
    "nnfs.init() ## Initialize nnfs (sets default data type to float32 etc.)\n",
    "\n",
    "## Create Layer class \n",
    "class Layer_Dense: \n",
    "    def __init__(self, n_inputs, n_neurons): ## n_inputs = number of inputs to the layer, n_neurons = number of neurons in the layer\n",
    "        ## Initialize weights and biases\n",
    "        self.weights = 0.10 * np.random.randn(n_inputs, n_neurons) ## Random weights with small values. This is useful for forward pass and we do not need to transpose weights here.\n",
    "        self.biases = np.zeros((1, n_neurons)) ## Biases should be initialized to zero values\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        ## Forward pass\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "\n",
    "\n",
    "class Activation_ReLU:\n",
    "    def forward(self, inputs):\n",
    "        ## Apply ReLU activation function\n",
    "        self.output = np.maximum(0, inputs) ## Element-wise maximum operation\n",
    "\n",
    "class softmax_Activation:\n",
    "    def forward(self, inputs):\n",
    "        ## Apply Softmax activation function\n",
    "        exp_values = np.exp(inputs - np.max(inputs, axis=1, keepdims=True)) ## Subtract max for numerical stability\n",
    "        probabilities= exp_values / np.sum(exp_values, axis=1, keepdims=True) ## Normalization step\n",
    "        self.output = probabilities\n",
    "\n",
    "class Loss_log:\n",
    "    def calculate(self, y_pred, y_true):\n",
    "        ## Calculate categorical cross-entropy loss\n",
    "        samples = y_pred.shape[0] ## Number of samples\n",
    "        y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7) ## Clip predictions to avoid log(0)\n",
    "        \n",
    "        ## Probabilities for target values\n",
    "        correct_confidences = y_pred_clipped[range(samples), y_true]\n",
    "        \n",
    "        negative_log_likelihoods = -np.log(correct_confidences) ## Negative log likelihoods\n",
    "        loss = np.mean(negative_log_likelihoods) ## Mean loss\n",
    "        return loss\n",
    "\n",
    "## Create first layer\n",
    "X, y = spiral_data(samples = 100, classes =3) ## Create dataset with 3 classes\n",
    "dense1 = Layer_Dense(2,3) ## Create first dense layer with 2 inputs and 3 neurons\n",
    "# Create activation function\n",
    "activation1 = Activation_ReLU() ## Create ReLU activation function object\n",
    "\n",
    "\n",
    "dense2 = Layer_Dense(3,3) ## Create second dense layer with 3 inputs (from previous layer) and 3 neurons\n",
    "activation2 = softmax_Activation() ## Create Softmax activation function object\n",
    "\n",
    "dense1.forward(X) ## Forward pass through first layer\n",
    "dense2.forward(dense1.output) ## Forward pass through second layer\n",
    "\n",
    "activation1.forward(dense1.output) ## Apply ReLU activation function to output of first layer\n",
    "activation2.forward(dense2.output) ## Apply Softmax activation function to output of second layer\n",
    "\n",
    "print(\"First layer output after ReLU activation = \\n\", activation1.output[:5]) ## Print output of first layer after ReLU activation\n",
    "print(\"\\n\")\n",
    "print(\"Second layer output after Softmax activation = \\n\", activation2.output[:5]) ## Print output of second layer after Softmax activation\n",
    "print(\"\\n\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7 0.5 0.9]\n",
      "Losses for each sample:  [0.35667494 0.69314718 0.10536052]\n",
      "Mean of losses:  0.38506088005216804\n"
     ]
    }
   ],
   "source": [
    "softmax_outputs = np.array([[0.7, 0.1, 0.2],\n",
    "                            [0.1, 0.5, 0.4],\n",
    "                            [0.02, 0.9, 0.08]]) # Predicted probabilities for 3 samples and 3 classes\n",
    "class_targets = np.array([0, 1, 1]) ## Class labels for each sample\n",
    "\n",
    "print(softmax_outputs[[0, 1, 2], class_targets]) ## Print predicted probabilities for true classes\n",
    "\n",
    "print('Losses for each sample: ',-np.log(softmax_outputs[[0, 1, 2], class_targets])) \n",
    "## Print losses for each sample. Smaller loss means the model is doing better as it is predicting higher probabilities for the true classes.\n",
    "\n",
    "print('Mean of losses: ', np.mean(-np.log(softmax_outputs[[0, 1, 2], class_targets])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First layer output after ReLU activation = \n",
      " [[0.         0.         0.        ]\n",
      " [0.         0.00113954 0.        ]\n",
      " [0.         0.00317292 0.        ]\n",
      " [0.         0.00526663 0.        ]\n",
      " [0.         0.00714014 0.        ]]\n",
      "\n",
      "\n",
      "Second layer output after Softmax activation = \n",
      " [[0.33333334 0.33333334 0.33333334]\n",
      " [0.33334687 0.33334196 0.3333112 ]\n",
      " [0.3333612  0.3333536  0.33328524]\n",
      " [0.33336097 0.33335987 0.33327916]\n",
      " [0.33337367 0.3333704  0.33325592]]\n",
      "\n",
      "\n",
      "Loss:  1.0984595\n",
      "Accuracy:  0.35\n"
     ]
    }
   ],
   "source": [
    "from nnfs.datasets import spiral_data\n",
    "np.random.seed(0) ## Set random seed for reproducibility\n",
    "\n",
    "nnfs.init() ## Initialize nnfs (sets default data type to float32 etc.)\n",
    "\n",
    "## Create Layer class \n",
    "class Layer_Dense: \n",
    "    def __init__(self, n_inputs, n_neurons): ## n_inputs = number of inputs to the layer, n_neurons = number of neurons in the layer\n",
    "        ## Initialize weights and biases\n",
    "        self.weights = 0.10 * np.random.randn(n_inputs, n_neurons) ## Random weights with small values. This is useful for forward pass and we do not need to transpose weights here.\n",
    "        self.biases = np.zeros((1, n_neurons)) ## Biases should be initialized to zero values\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        ## Forward pass\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "\n",
    "\n",
    "class Activation_ReLU:\n",
    "    def forward(self, inputs):\n",
    "        ## Apply ReLU activation function\n",
    "        self.output = np.maximum(0, inputs) ## Element-wise maximum operation\n",
    "\n",
    "class softmax_Activation:\n",
    "    def forward(self, inputs):\n",
    "        ## Apply Softmax activation function\n",
    "        exp_values = np.exp(inputs - np.max(inputs, axis=1, keepdims=True)) ## Subtract max for numerical stability\n",
    "        probabilities= exp_values / np.sum(exp_values, axis=1, keepdims=True) ## Normalization step\n",
    "        self.output = probabilities\n",
    "\n",
    "class Loss:\n",
    "    def calculate(self, output, y):\n",
    "        ## Calculate loss\n",
    "        sample_losses = self.forward(output, y) ## Forward pass to calculate losses, it will vary depending on the loss function\n",
    "        data_loss = np.mean(sample_losses) ## Mean loss\n",
    "        return data_loss\n",
    "    \n",
    "class Loss_CategoricalCrossentropy(Loss):\n",
    "    def forward(self, y_pred, y_true): # y_pred values from NN, y_true are target training values\n",
    "        ## Calculate categorical cross-entropy loss\n",
    "        samples = y_pred.shape[0] ## Number of samples\n",
    "        y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7) \n",
    "        ## Clip predictions to avoid log(0). We use 1-1e-7 to avoid biasing towards any class which means we do not allow any predicted probability to be exactly 1.\n",
    "        \n",
    "        if len(y_true.shape) == 1: ##\n",
    "            correct_confidences = y_pred_clipped[range(samples), y_true]\n",
    "        elif len(y_true.shape) == 2: ## One hot encoded vector\n",
    "            correct_confidences = np.sum(y_pred_clipped * y_true, axis=1) \n",
    "        ## Probabilities for target values\n",
    "        correct_confidences = y_pred_clipped[range(samples), y_true]\n",
    "        \n",
    "        negative_log_likelihoods = -np.log(correct_confidences) ## Negative log likelihoods\n",
    "        return negative_log_likelihoods\n",
    "    \n",
    "\n",
    "## Create first layer\n",
    "X, y = spiral_data(samples = 100, classes =3) ## Create dataset with 3 classes\n",
    "dense1 = Layer_Dense(2,3) ## Create first dense layer with 2 inputs and 3 neurons\n",
    "# Create activation function\n",
    "activation1 = Activation_ReLU() ## Create ReLU activation function object\n",
    "\n",
    "\n",
    "dense2 = Layer_Dense(3,3) ## Create second dense layer with 3 inputs (from previous layer) and 3 neurons\n",
    "activation2 = softmax_Activation() ## Create Softmax activation function object\n",
    "\n",
    "dense1.forward(X) ## Forward pass through first layer\n",
    "dense2.forward(dense1.output) ## Forward pass through second layer\n",
    "\n",
    "activation1.forward(dense1.output) ## Apply ReLU activation function to output of first layer\n",
    "activation2.forward(dense2.output) ## Apply Softmax activation function to output of second layer\n",
    "\n",
    "print(\"First layer output after ReLU activation = \\n\", activation1.output[:5]) ## Print output of first layer after ReLU activation\n",
    "print(\"\\n\")\n",
    "print(\"Second layer output after Softmax activation = \\n\", activation2.output[:5]) ## Print output of second layer after Softmax activation\n",
    "print(\"\\n\") \n",
    "\n",
    "loss_function = Loss_CategoricalCrossentropy() ## Create loss function object\n",
    "loss = loss_function.calculate(activation2.output, y) ## Calculate loss, so you need to pass the y prediction and true labels\n",
    "print(\"Loss: \", loss) ## Print loss\n",
    "\n",
    "# Calculating accuracy not as usueful as loss but good to see\n",
    "predictions = np.argmax(activation2.output, axis=1) ## this gives the index of the highest predicted probability for each sample\n",
    "accuracy = np.mean(predictions == y) ## this compares the predictions to the true labels and calculates the mean accuracy\n",
    "print(\"Accuracy: \", accuracy) ## Print accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mSignature:\u001b[39m       np.argmax(a, axis=\u001b[38;5;28;01mNone\u001b[39;00m, out=\u001b[38;5;28;01mNone\u001b[39;00m, *, keepdims=<no value>)\n",
      "\u001b[31mCall signature:\u001b[39m  np.argmax(*args, **kwargs)\n",
      "\u001b[31mType:\u001b[39m            _ArrayFunctionDispatcher\n",
      "\u001b[31mString form:\u001b[39m     <function argmax at 0x10edab2e0>\n",
      "\u001b[31mFile:\u001b[39m            ~/miniconda3/envs/mlproj/lib/python3.12/site-packages/numpy/_core/fromnumeric.py\n",
      "\u001b[31mDocstring:\u001b[39m      \n",
      "Returns the indices of the maximum values along an axis.\n",
      "\n",
      "Parameters\n",
      "----------\n",
      "a : array_like\n",
      "    Input array.\n",
      "axis : int, optional\n",
      "    By default, the index is into the flattened array, otherwise\n",
      "    along the specified axis.\n",
      "out : array, optional\n",
      "    If provided, the result will be inserted into this array. It should\n",
      "    be of the appropriate shape and dtype.\n",
      "keepdims : bool, optional\n",
      "    If this is set to True, the axes which are reduced are left\n",
      "    in the result as dimensions with size one. With this option,\n",
      "    the result will broadcast correctly against the array.\n",
      "\n",
      "    .. versionadded:: 1.22.0\n",
      "\n",
      "Returns\n",
      "-------\n",
      "index_array : ndarray of ints\n",
      "    Array of indices into the array. It has the same shape as ``a.shape``\n",
      "    with the dimension along `axis` removed. If `keepdims` is set to True,\n",
      "    then the size of `axis` will be 1 with the resulting array having same\n",
      "    shape as ``a.shape``.\n",
      "\n",
      "See Also\n",
      "--------\n",
      "ndarray.argmax, argmin\n",
      "amax : The maximum value along a given axis.\n",
      "unravel_index : Convert a flat index into an index tuple.\n",
      "take_along_axis : Apply ``np.expand_dims(index_array, axis)``\n",
      "                  from argmax to an array as if by calling max.\n",
      "\n",
      "Notes\n",
      "-----\n",
      "In case of multiple occurrences of the maximum values, the indices\n",
      "corresponding to the first occurrence are returned.\n",
      "\n",
      "Examples\n",
      "--------\n",
      ">>> import numpy as np\n",
      ">>> a = np.arange(6).reshape(2,3) + 10\n",
      ">>> a\n",
      "array([[10, 11, 12],\n",
      "       [13, 14, 15]])\n",
      ">>> np.argmax(a)\n",
      "5\n",
      ">>> np.argmax(a, axis=0)\n",
      "array([1, 1, 1])\n",
      ">>> np.argmax(a, axis=1)\n",
      "array([2, 2])\n",
      "\n",
      "Indexes of the maximal elements of a N-dimensional array:\n",
      "\n",
      ">>> ind = np.unravel_index(np.argmax(a, axis=None), a.shape)\n",
      ">>> ind\n",
      "(1, 2)\n",
      ">>> a[ind]\n",
      "15\n",
      "\n",
      ">>> b = np.arange(6)\n",
      ">>> b[1] = 5\n",
      ">>> b\n",
      "array([0, 5, 2, 3, 4, 5])\n",
      ">>> np.argmax(b)  # Only the first occurrence is returned.\n",
      "1\n",
      "\n",
      ">>> x = np.array([[4,2,3], [1,0,3]])\n",
      ">>> index_array = np.argmax(x, axis=-1)\n",
      ">>> # Same as np.amax(x, axis=-1, keepdims=True)\n",
      ">>> np.take_along_axis(x, np.expand_dims(index_array, axis=-1), axis=-1)\n",
      "array([[4],\n",
      "       [3]])\n",
      ">>> # Same as np.amax(x, axis=-1)\n",
      ">>> np.take_along_axis(x, np.expand_dims(index_array, axis=-1),\n",
      "...     axis=-1).squeeze(axis=-1)\n",
      "array([4, 3])\n",
      "\n",
      "Setting `keepdims` to `True`,\n",
      "\n",
      ">>> x = np.arange(24).reshape((2, 3, 4))\n",
      ">>> res = np.argmax(x, axis=1, keepdims=True)\n",
      ">>> res.shape\n",
      "(2, 1, 4)\n",
      "\u001b[31mClass docstring:\u001b[39m\n",
      "Class to wrap functions with checks for __array_function__ overrides.\n",
      "\n",
      "All arguments are required, and can only be passed by position.\n",
      "\n",
      "Parameters\n",
      "----------\n",
      "dispatcher : function or None\n",
      "    The dispatcher function that returns a single sequence-like object\n",
      "    of all arguments relevant.  It must have the same signature (except\n",
      "    the default values) as the actual implementation.\n",
      "    If ``None``, this is a ``like=`` dispatcher and the\n",
      "    ``_ArrayFunctionDispatcher`` must be called with ``like`` as the\n",
      "    first (additional and positional) argument.\n",
      "implementation : function\n",
      "    Function that implements the operation on NumPy arrays without\n",
      "    overrides.  Arguments passed calling the ``_ArrayFunctionDispatcher``\n",
      "    will be forwarded to this (and the ``dispatcher``) as if using\n",
      "    ``*args, **kwargs``.\n",
      "\n",
      "Attributes\n",
      "----------\n",
      "_implementation : function\n",
      "    The original implementation passed in."
     ]
    }
   ],
   "source": [
    "np.argmax?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlproj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
